import argparse
import datetime
import numpy as np
import time
import random
import torch
import torch.backends.cudnn as cudnn
import torch.nn as nn
from torch import optim
import json
import os
from contextlib import suppress
import random
from pathlib import Path 
from collections import OrderedDict
import copy
import utils.utils as utils
from utils.build_dataset import build_dataset
from utils.multi_model3 import DualModelManager
from utils.utils import NativeScalerWithGradNormCount as NativeScaler
from engine_dual_training9 import train_one_epoch_dual, evaluate_dual
from utils.center14 import build_memory
import warnings
from utils.capability_probe2 import CapabilityGapProbe
warnings.filterwarnings("ignore")

def get_args():
    parser = argparse.ArgumentParser('Dual model training with adaptive distillation', add_help=False)
    parser.add_argument('--batch_size', default=64, type=int)
    parser.add_argument('--save_ckpt_freq', default=10, type=int)
    parser.add_argument('--eval_freq', default=1, type=int) 
    
    # è®­ç»ƒç­–ç•¥é…ç½®
    parser.add_argument('--training_strategy', type=str, default=None,
                       help='Training strategy as comma-separated string, e.g., "T,T,T,S,S,K1,K1,K2,K2"')
    
    # æ•™å¸ˆæ¨¡å‹é€‰æ‹©é…ç½®
    parser.add_argument('--choose_teacher_model', type=str, default=None,
                       help='Teacher model sequence to use for distillation, e.g., "1,2,3" or "8,9,10"')
    
    # è’¸é¦logitsç±»å‹é€‰æ‹©é…ç½®
    parser.add_argument('--choose_logits', type=str, default='1,2,5',
                       help='Logits types for distillation as comma-separated string, e.g., "1,2,3,4,5". '
                            '1: weakÃ—text, 2: strongÃ—text, 3: visualÃ—text prototype, 4: pseudo-label, 5: DKD loss')
    
    # è‡ªé€‚åº”è’¸é¦é…ç½®
    parser.add_argument('--use_adaptive_distillation', action='store_true',
                       help='Use adaptive temperature and alpha based on Kendall correlation')
    parser.add_argument('--no_adaptive_distillation', dest='use_adaptive_distillation', action='store_false')
    parser.set_defaults(use_adaptive_distillation=True)
    
    # æ¸©åº¦å’ŒAlphaèŒƒå›´é…ç½®
    parser.add_argument('--temp_range', type=str, default='2.0,8.0',
                       help='Temperature range for adaptive distillation as "min,max"')
    parser.add_argument('--alpha_range', type=str, default='0.3,0.9',
                       help='Alpha range for adaptive distillation as "min,max"')
    
    # è’¸é¦æŸå¤±æƒé‡é…ç½®
    parser.add_argument('--logits_weights', type=str, default='1.0,1.0,1.0,1.0,1.0',
                       help='Weights for different logits types as comma-separated string')
    
    # DKDç‰¹å®šå‚æ•°
    parser.add_argument('--tkl_weight', type=float, default=1.0,
                       help='Target Knowledge Loss weight for DKD')
    parser.add_argument('--ntkl_weight', type=float, default=8.0,
                       help='Non-Target Knowledge Loss weight for DKD')
    
    parser.add_argument('--kendall_threshold', type=float, default=None,
                    help='Kendall correlation threshold for sample filtering (optional, for backward compatibility)')
    

    parser.add_argument('--teacher_backbone', type=str, default='ViT-L/14',
                       choices=['ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px',
                               'RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64'],
                       help='Teacher model backbone')
    
    parser.add_argument('--student_backbone', type=str, default='ViT-B/32',
                       choices=['ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px',
                               'RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64'],
                       help='Student model backbone')



    # CLIP parameters
    parser.add_argument("--template", default='templates.json', type=str)
    parser.add_argument("--classname", default='classes.json', type=str)
    parser.add_argument('--image_mean', default=(0.48145466, 0.4578275, 0.40821073)) 
    parser.add_argument('--image_std', default=(0.26862954, 0.26130258, 0.27577711)) 
    parser.add_argument('--input_size', default=224, type=int, help='images input size') 
    
    # training parameters
    parser.add_argument("--train_config", default='train_configs.json', type=str)
    
    # Optimizer parameters
    parser.add_argument('--momentum', type=float, default=0.9, metavar='M')
    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER')
    parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON')
    parser.add_argument('--opt_betas', default=None, type=float, nargs='+', metavar='BETA')
    parser.add_argument('--weight_decay', type=float, default=0.05)
    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR')
    parser.add_argument('--layer_decay', type=float, default=0.65)
    parser.add_argument('--warmup_lr', type=float, default=1e-6, metavar='LR')
    parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR')
    parser.add_argument('--warmup_epochs', type=int, default=0, metavar='N')
    parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N')
    
    # Augmentation parameters  
    parser.add_argument('--train_crop_min', default=0.3, type=float)
    parser.add_argument('--color_jitter', type=float, default=0, metavar='PCT')
    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME')
    parser.add_argument('--train_interpolation', type=str, default='bicubic')
    
    # Dataset parameters
    parser.add_argument('--nb_classes', default=0, type=int)
    parser.add_argument('--dataset', default='imagenet', type=str)
    parser.add_argument('--output_dir', default='', help='path to save checkpoint and log')
    parser.add_argument('--device', default='cuda:0', type=str, 
                       help='device to use for training (e.g., cuda:0, cuda:1)')
    parser.add_argument('--seed', default=0, type=int)
    parser.add_argument('--resume', default='')
    parser.add_argument('--auto_resume', action='store_true')
    parser.set_defaults(auto_resume=True)
    parser.add_argument('--start_epoch', default=0, type=int, metavar='N')
    parser.add_argument('--eval', action='store_true')
    parser.add_argument('--num_workers', default=10, type=int)
    parser.add_argument('--amp', action='store_true')
    
    return parser.parse_args()

def parse_training_strategy(strategy_str):
    """è§£æè®­ç»ƒç­–ç•¥å­—ç¬¦ä¸²"""
    if strategy_str is None:
        return None
    
    strategy_list = [s.strip().upper() for s in strategy_str.split(',')]
    
    # éªŒè¯ç­–ç•¥æœ‰æ•ˆæ€§
    valid_modes = {'T', 'S', 'K1', 'K2'}
    for mode in strategy_list:
        if mode not in valid_modes:
            raise ValueError(f"æ— æ•ˆçš„è®­ç»ƒæ¨¡å¼: {mode}. æœ‰æ•ˆæ¨¡å¼: {valid_modes}")
    
    return strategy_list

def parse_teacher_model_sequence(sequence_str):
    """è§£ææ•™å¸ˆæ¨¡å‹é€‰æ‹©åºåˆ—"""
    if sequence_str is None:
        return None
    
    try:
        sequence_list = [int(s.strip()) for s in sequence_str.split(',')]
        return sequence_list
    except ValueError as e:
        raise ValueError(f"æ— æ•ˆçš„æ•™å¸ˆæ¨¡å‹åºåˆ—: {sequence_str}. åº”è¯¥æ˜¯é€—å·åˆ†éš”çš„æ•°å­—ï¼Œå¦‚ '1,2,3'")

def parse_choose_logits(logits_str):
    """è§£æè’¸é¦logitsç±»å‹é€‰æ‹©"""
    if logits_str is None:
        return [5]  # é»˜è®¤ä½¿ç”¨DKDæŸå¤±
    
    try:
        logits_list = [int(s.strip()) for s in logits_str.split(',')]
        # éªŒè¯logitsç±»å‹æœ‰æ•ˆæ€§
        valid_logits = {1, 2, 3, 4, 5}
        for logit_type in logits_list:
            if logit_type not in valid_logits:
                raise ValueError(f"æ— æ•ˆçš„logitsç±»å‹: {logit_type}. æœ‰æ•ˆç±»å‹: {valid_logits}")
        return logits_list
    except ValueError as e:
        raise ValueError(f"æ— æ•ˆçš„logitsé€‰æ‹©: {logits_str}. åº”è¯¥æ˜¯é€—å·åˆ†éš”çš„æ•°å­—ï¼Œå¦‚ '1,2,3,4,5'")

def parse_logits_weights(weights_str):
    """è§£æè’¸é¦æŸå¤±æƒé‡"""
    if weights_str is None:
        return [1.0, 1.0, 1.0, 1.0, 1.0]  # é»˜è®¤æƒé‡
    
    try:
        weights_list = [float(s.strip()) for s in weights_str.split(',')]
        if len(weights_list) != 5:
            raise ValueError(f"æƒé‡æ•°é‡åº”ä¸º5ä¸ªï¼Œå®é™…ä¸º{len(weights_list)}ä¸ª")
        return weights_list
    except ValueError as e:
        raise ValueError(f"æ— æ•ˆçš„æƒé‡é…ç½®: {weights_str}. åº”è¯¥æ˜¯5ä¸ªé€—å·åˆ†éš”çš„æµ®ç‚¹æ•°ï¼Œå¦‚ '1.0,1.0,1.0,1.0,1.0'")

def parse_range(range_str):
    """è§£æèŒƒå›´å­—ç¬¦ä¸² (ä¾‹å¦‚ "2.0,8.0")"""
    try:
        parts = [float(s.strip()) for s in range_str.split(',')]
        if len(parts) != 2:
            raise ValueError(f"èŒƒå›´åº”åŒ…å«ä¸¤ä¸ªå€¼ï¼Œå®é™…ä¸º{len(parts)}ä¸ª")
        return tuple(parts)
    except ValueError as e:
        raise ValueError(f"æ— æ•ˆçš„èŒƒå›´é…ç½®: {range_str}. åº”è¯¥æ˜¯ä¸¤ä¸ªé€—å·åˆ†éš”çš„æ•°å­—ï¼Œå¦‚ '2.0,8.0'")

def get_next_teacher_sequence_number(dataset_name):
    """è·å–ä¸‹ä¸€ä¸ªæ•™å¸ˆæ¨¡å‹çš„åºåˆ—å·"""
    pth_dir = Path(f"./pth00/{dataset_name}")
    if not pth_dir.exists():
        return 1
    
    # æŸ¥æ‰¾æ‰€æœ‰æ•™å¸ˆæƒé‡æ–‡ä»¶
    teacher_files = list(pth_dir.glob("*T.pth"))
    if not teacher_files:
        return 1
    
    # æå–åºåˆ—å·å¹¶æ‰¾åˆ°æœ€å¤§å€¼
    max_sequence = 0
    for file in teacher_files:
        try:
            sequence_num = int(file.stem.replace('T', ''))
            max_sequence = max(max_sequence, sequence_num)
        except ValueError:
            continue
    
    return max_sequence + 1

def save_teacher_weights_sequential(teacher_model, dataset_name, training_mode):
    """æŒ‰é¡ºåºä¿å­˜æ•™å¸ˆæ¨¡å‹æƒé‡"""
    if training_mode != 'T':
        return None
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(f"./pth0/{dataset_name}")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–ä¸‹ä¸€ä¸ªåºåˆ—å·
    sequence_num = get_next_teacher_sequence_number(dataset_name)
    
    # ä¿å­˜æƒé‡æ–‡ä»¶ï¼Œæ–‡ä»¶åæ ¼å¼ï¼š{sequence_num}T.pth
    save_path = save_dir / f"{sequence_num}T.pth"
    
    # ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸
    torch.save({
        'model_state_dict': teacher_model.state_dict(),
        'sequence_num': sequence_num,
        'training_mode': training_mode,
        'center': teacher_model.center.clone() if hasattr(teacher_model, 'center') else None
    }, save_path)
    
    print(f"âœ… æ•™å¸ˆæ¨¡å‹æƒé‡å·²ä¿å­˜: {save_path} (åºåˆ—å·: {sequence_num})")
    return sequence_num

def load_teacher_weights_by_sequence(teacher_model, dataset_name, sequence_num):
    """æ ¹æ®åºåˆ—å·åŠ è½½æ•™å¸ˆæ¨¡å‹æƒé‡"""
    load_path = Path(f"./pth0/{dataset_name}/{sequence_num}T.pth")
    
    if not load_path.exists():
        print(f"âŒ æ•™å¸ˆæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
        return False
    
    try:
        checkpoint = torch.load(load_path, map_location='cpu')
        teacher_model.load_state_dict(checkpoint['model_state_dict'])
        
        # å¦‚æœä¿å­˜äº†centerï¼Œä¹ŸåŠ è½½å®ƒ
        if checkpoint.get('center') is not None and hasattr(teacher_model, 'center'):
            teacher_model.center = checkpoint['center']
        
        print(f"âœ… æˆåŠŸåŠ è½½æ•™å¸ˆæƒé‡: {load_path} (åºåˆ—å·: {sequence_num})")
        return True
    except Exception as e:
        print(f"âŒ åŠ è½½æ•™å¸ˆæƒé‡å¤±è´¥: {load_path}, é”™è¯¯: {e}")
        return False

def check_and_load_existing_teacher_weights_by_sequence(dual_model, dataset_name, sequence_num):
    """æ£€æŸ¥å¹¶åŠ è½½æŒ‡å®šåºåˆ—å·çš„æ•™å¸ˆæƒé‡"""
    load_path = Path(f"./pth0/{dataset_name}/{sequence_num}T.pth")
    
    if load_path.exists():
        try:
            checkpoint = torch.load(load_path, map_location='cpu')
            dual_model.teacher.load_state_dict(checkpoint['model_state_dict'])
            
            # å¦‚æœä¿å­˜äº†centerï¼Œä¹ŸåŠ è½½å®ƒ
            if checkpoint.get('center') is not None and hasattr(dual_model.teacher, 'center'):
                dual_model.teacher.center = checkpoint['center']
            
            print(f"âœ… å‘ç°å¹¶åŠ è½½å·²å­˜åœ¨çš„æ•™å¸ˆæƒé‡: {load_path} (åºåˆ—å·: {sequence_num})")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ•™å¸ˆæƒé‡å¤±è´¥: {load_path}, é”™è¯¯: {e}")
            return False
    else:
        print(f"ğŸ“ æ•™å¸ˆæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
        return False

def get_available_teacher_sequences(dataset_name):
    """è·å–å¯ç”¨çš„æ•™å¸ˆæ¨¡å‹åºåˆ—å·åˆ—è¡¨"""
    pth_dir = Path(f"./pth0/{dataset_name}")
    if not pth_dir.exists():
        return []
    
    teacher_files = list(pth_dir.glob("*T.pth"))
    sequences = []
    
    for file in teacher_files:
        try:
            sequence_num = int(file.stem.replace('T', ''))
            sequences.append(sequence_num)
        except ValueError:
            continue
    
    return sorted(sequences)

def build_teacher_usage_plan(training_strategy, teacher_sequence):
    """æ„å»ºæ•™å¸ˆæ¨¡å‹ä½¿ç”¨è®¡åˆ’"""
    if not training_strategy or not teacher_sequence:
        return {}
    
    # æ‰¾åˆ°æ‰€æœ‰éœ€è¦è’¸é¦çš„epoch
    distill_epochs = []
    for epoch, mode in enumerate(training_strategy):
        if mode in ['K1', 'K2']:
            distill_epochs.append(epoch)
    
    # æ£€æŸ¥æ•™å¸ˆåºåˆ—æ•°é‡æ˜¯å¦åŒ¹é…
    if len(distill_epochs) != len(teacher_sequence):
        raise ValueError(
            f"è’¸é¦epochæ•°é‡ ({len(distill_epochs)}) ä¸æŒ‡å®šçš„æ•™å¸ˆæ¨¡å‹æ•°é‡ ({len(teacher_sequence)}) ä¸åŒ¹é…ï¼\n"
            f"éœ€è¦è’¸é¦çš„epoch: {distill_epochs}\n"
            f"æŒ‡å®šçš„æ•™å¸ˆåºåˆ—: {teacher_sequence}\n"
            f"è¯·ç¡®ä¿ choose_teacher_model å‚æ•°åŒ…å« {len(distill_epochs)} ä¸ªæ•™å¸ˆæ¨¡å‹åºåˆ—å·"
        )
    
    # æ„å»ºepochåˆ°æ•™å¸ˆåºåˆ—å·çš„ä¸€ä¸€å¯¹åº”æ˜ å°„
    teacher_plan = {}
    for i, epoch in enumerate(distill_epochs):
        teacher_plan[epoch] = teacher_sequence[i]
    
    return teacher_plan

def setup_optimizers(dual_model, train_config):
    """ä¸ºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹åˆ†åˆ«è®¾ç½®ä¼˜åŒ–å™¨"""
    
    def get_params(model):
        params = []
        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
        
        trainable_params = []
        for name, param in model.named_parameters():
            if param.requires_grad:
                trainable_params.append((name, param))
        
        optimizer_grouped_parameters = [
            {'params': [p for n, p in trainable_params
                       if not any(nd in n for nd in no_decay)],
             'weight_decay': 0.1},
            {'params': [p for n, p in trainable_params
                       if any(nd in n for nd in no_decay)], 
             'weight_decay': 0.0}
        ]
        return optimizer_grouped_parameters
    
    # æ•™å¸ˆæ¨¡å‹ä¼˜åŒ–å™¨
    teacher_params = get_params(dual_model.teacher)
    teacher_optimizer = optim.AdamW(teacher_params, lr=train_config['lr'])
    
    # å­¦ç”Ÿæ¨¡å‹ä¼˜åŒ–å™¨
    student_params = get_params(dual_model.student)
    student_optimizer = optim.AdamW(student_params, lr=train_config['lr'])
    
    return teacher_optimizer, student_optimizer

def main(args):

    # GPUè®¾ç½®å’ŒéªŒè¯
    print(f"ğŸ¯ æŒ‡å®šä½¿ç”¨è®¾å¤‡: {args.device}")
    
    # è®¾ç½®CUDAè®¾å¤‡
    if 'cuda:' in args.device:
        gpu_id = int(args.device.split(':')[1])
        if torch.cuda.is_available():
            if gpu_id >= torch.cuda.device_count():
                raise ValueError(f"æŒ‡å®šçš„GPU {gpu_id} ä¸å­˜åœ¨ï¼å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
            torch.cuda.set_device(gpu_id)
            print(f"ğŸ¯ è®¾ç½®å½“å‰CUDAè®¾å¤‡ä¸ºGPU {gpu_id}")
        else:
            raise RuntimeError("CUDAä¸å¯ç”¨ï¼")
    
    device = torch.device(args.device)
    
    # éªŒè¯GPUä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        current_device = torch.cuda.current_device()
        device_name = torch.cuda.get_device_name(current_device)
        print(f"ğŸ¯ å½“å‰ä½¿ç”¨GPU {current_device}: {device_name}")
        print(f"ğŸ¯ å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
        
        # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_allocated = torch.cuda.memory_allocated(current_device) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(current_device) / 1024**3
        print(f"ğŸ¯ GPU {current_device} å†…å­˜ä½¿ç”¨: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB")
    
    # è§£æè®­ç»ƒç­–ç•¥å’Œæ•™å¸ˆæ¨¡å‹ä½¿ç”¨è®¡åˆ’
    if args.training_strategy:
        args.training_strategy = parse_training_strategy(args.training_strategy)
        print(f"\nä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒç­–ç•¥: {args.training_strategy}")
        
        # ç»Ÿè®¡éœ€è¦è’¸é¦çš„epochæ•°é‡å’Œæ•™å¸ˆè®­ç»ƒçš„epochæ•°é‡
        distill_epochs = [i for i, mode in enumerate(args.training_strategy) if mode in ['K1', 'K2']]
        teacher_epochs = [i for i, mode in enumerate(args.training_strategy) if mode == 'T']
        
        if distill_epochs:
            print(f"éœ€è¦è’¸é¦çš„epoch: {distill_epochs} (å…±{len(distill_epochs)}ä¸ª)")
        if teacher_epochs:
            print(f"éœ€è¦æ•™å¸ˆè®­ç»ƒçš„epoch: {teacher_epochs} (å…±{len(teacher_epochs)}ä¸ª)")

    # è§£ææ•™å¸ˆæ¨¡å‹é€‰æ‹©åºåˆ—
    teacher_sequence = None
    teacher_usage_plan = {}
    teacher_training_plan = {}
    
    if args.choose_teacher_model:
        teacher_sequence = parse_teacher_model_sequence(args.choose_teacher_model)
        print(f"\næŒ‡å®šæ•™å¸ˆæ¨¡å‹åºåˆ—: {teacher_sequence} (å…±{len(teacher_sequence)}ä¸ª)")
        
        if args.training_strategy:
            try:
                teacher_usage_plan = build_teacher_usage_plan(args.training_strategy, teacher_sequence)
                print("æ•™å¸ˆæ¨¡å‹ä½¿ç”¨è®¡åˆ’:")
                for epoch, seq_num in teacher_usage_plan.items():
                    mode = args.training_strategy[epoch]
                    print(f"  Epoch {epoch} ({mode}): ä½¿ç”¨æ•™å¸ˆæ¨¡å‹ {seq_num}T.pth")
                
                # æ„å»ºæ•™å¸ˆè®­ç»ƒè®¡åˆ’
                teacher_epochs = [i for i, mode in enumerate(args.training_strategy) if mode == 'T']
                if teacher_epochs:
                    needed_teachers = set(teacher_sequence)
                    available_teachers = set(get_available_teacher_sequences(args.dataset))
                    missing_teachers = needed_teachers - available_teachers
                    
                    if missing_teachers:
                        missing_teachers = sorted(missing_teachers)
                        print(f"\néœ€è¦è®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹: {missing_teachers}")
                        
                        for i, epoch in enumerate(teacher_epochs):
                            if i < len(missing_teachers):
                                teacher_training_plan[epoch] = missing_teachers[i]
                        
                        print("æ•™å¸ˆè®­ç»ƒè®¡åˆ’:")
                        for epoch, seq_num in teacher_training_plan.items():
                            print(f"  Epoch {epoch} (T): è®­ç»ƒæ•™å¸ˆæ¨¡å‹ {seq_num}T.pth")
                    else:
                        print("\nâœ… æ‰€æœ‰éœ€è¦çš„æ•™å¸ˆæ¨¡å‹éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€è®­ç»ƒæ–°çš„æ•™å¸ˆæ¨¡å‹")
                        
            except ValueError as e:
                print(f"âŒ æ•™å¸ˆæ¨¡å‹é…ç½®é”™è¯¯: {e}")
                return

    # è§£æè’¸é¦logitsç±»å‹é€‰æ‹©
    choose_logits = parse_choose_logits(args.choose_logits)
    print(f"\nğŸ¯ è’¸é¦logitsç±»å‹é…ç½®: {choose_logits}")
    logits_type_descriptions = {
        1: "å¼±å¢å¼ºÃ—æ–‡æœ¬åŸå‹è’¸é¦",
        2: "å¼ºå¢å¼ºÃ—æ–‡æœ¬åŸå‹è’¸é¦", 
        3: "è§†è§‰åŸå‹Ã—æ–‡æœ¬åŸå‹è’¸é¦",
        4: "ä¼ªæ ‡ç­¾è’¸é¦",
        5: "DKDæŸå¤±(è§£è€¦çŸ¥è¯†è’¸é¦)"
    }
    for logit_type in choose_logits:
        print(f"  ç±»å‹ {logit_type}: {logits_type_descriptions[logit_type]}")

    # è§£æè’¸é¦æŸå¤±æƒé‡
    logits_weights = parse_logits_weights(args.logits_weights)
    print(f"\nğŸ¯ è’¸é¦æŸå¤±æƒé‡é…ç½®: {logits_weights}")
    for i, weight in enumerate(logits_weights, 1):
        if i in choose_logits:
            print(f"  ç±»å‹ {i} æƒé‡: {weight} âœ“ (å¯ç”¨)")
        else:
            print(f"  ç±»å‹ {i} æƒé‡: {weight} (æœªå¯ç”¨)")

    # è§£ææ¸©åº¦å’ŒalphaèŒƒå›´
    temp_range = parse_range(args.temp_range)
    alpha_range = parse_range(args.alpha_range)
    
    print(f"\nğŸ¯ è‡ªé€‚åº”è’¸é¦é…ç½®:")
    print(f"  å¯ç”¨è‡ªé€‚åº”è’¸é¦: {args.use_adaptive_distillation}")
    if args.use_adaptive_distillation:
        print(f"  æ¸©åº¦èŒƒå›´: {temp_range}")
        print(f"  AlphaèŒƒå›´: {alpha_range}")

    # ä¿å­˜æ—¥å¿—
    log_path = os.path.join(args.output_dir, f"{args.dataset}_dual_adaptive_trainlog.txt")
    Path(args.output_dir).parent.mkdir(parents=True, exist_ok=True)
    
    # è®°å½•è®­ç»ƒç­–ç•¥åˆ°æ—¥å¿—
    log_args = dict(args._get_kwargs())
    if args.training_strategy:
        log_args['training_strategy'] = ','.join(args.training_strategy)
    if args.choose_teacher_model:
        log_args['choose_teacher_model'] = args.choose_teacher_model
    log_args['choose_logits'] = ','.join(map(str, choose_logits))
    log_args['logits_weights'] = ','.join(map(str, logits_weights))
    log_args['use_adaptive_distillation'] = args.use_adaptive_distillation
    log_args['temp_range'] = ','.join(map(str, temp_range))
    log_args['alpha_range'] = ','.join(map(str, alpha_range))
    
    with open(log_path, mode="a", encoding="utf-8") as f:
        f.write(json.dumps(log_args) + "\n")
    


    # è®­ç»ƒé…ç½®
    train_config_path = os.path.join("./json_files", args.train_config)
    with open(train_config_path, 'r') as train_config_file:
        train_config_data = json.load(train_config_file)
    
    # ä½¿ç”¨ViT-L/14çš„é…ç½®ä½œä¸ºåŸºç¡€é…ç½®
    train_config = train_config_data[args.dataset + '_ViT-L/14']
    
    # æ·»åŠ è’¸é¦é…ç½®ï¼ˆä¿æŒä¸train16.pyç›¸åŒçš„é€»è¾‘ï¼‰
    train_config.update({
        'distill_temp': 4.0,           # è’¸é¦æ¸©åº¦
        'dis_weight': 1.0,             # è’¸é¦æŸå¤±æƒé‡
        'dpa_weight': 1.0,             # K2æ¨¡å¼ä¸‹DPAæŸå¤±çš„æƒé‡
        'choose_logits': choose_logits,  # é€‰æ‹©çš„è’¸é¦logitsç±»å‹
        'logits_weights': logits_weights,  # è’¸é¦æŸå¤±æƒé‡
        'tkl_weight': args.tkl_weight,   # DKDç›®æ ‡çŸ¥è¯†æŸå¤±æƒé‡
        'ntkl_weight': args.ntkl_weight,  # DKDéç›®æ ‡çŸ¥è¯†æŸå¤±æƒé‡
        
        # æ–°å¢ï¼šè‡ªé€‚åº”è’¸é¦é…ç½®
        'use_adaptive_distillation': args.use_adaptive_distillation,
        'temp_range': temp_range,      # æ¸©åº¦èŒƒå›´ (min, max)
        'alpha_range': alpha_range,    # AlphaèŒƒå›´ (min, max)
    })

    # è®¾ç½®è‚¯å¾·å°”ç›¸å…³ç³»æ•°é˜ˆå€¼ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼Œä¿æŒå…¼å®¹ï¼‰
    if args.kendall_threshold is not None:
        train_config['kendall_threshold'] = args.kendall_threshold
    else:
        dataset_kendall_thresholds = {
            'cars': 0.5, 'dtd': 0.0, 'fgvc': 0.6, 'food101': 0.0,
            'flowers': 0.5, 'pets': 0.5, 'aircraft': 0.0,
            'caltech101': 0.0, 'eurosat': 0.0, 'ucf101': 0.0,
        }
        train_config['kendall_threshold'] = dataset_kendall_thresholds.get(args.dataset, 0.0)

    # æ‰“å°é…ç½®æ‘˜è¦
    print("\n" + "="*80)
    print("ğŸ¯ è‡ªé€‚åº”çŸ¥è¯†è’¸é¦é…ç½®æ‘˜è¦:")
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  å¯ç”¨è‡ªé€‚åº”è’¸é¦: {train_config['use_adaptive_distillation']}")
    if train_config['use_adaptive_distillation']:
        print(f"  æ¸©åº¦èŒƒå›´: {train_config['temp_range']}")
        print(f"  AlphaèŒƒå›´: {train_config['alpha_range']}")
    print(f"  è’¸é¦æ¸©åº¦: {train_config['distill_temp']}")
    print(f"  è’¸é¦æƒé‡: {train_config['dis_weight']}")
    print(f"  DPAæƒé‡(K2æ¨¡å¼): {train_config['dpa_weight']}")
    if 5 in choose_logits:
        print(f"  DKD-TKLæƒé‡: {train_config['tkl_weight']}")
        print(f"  DKD-NTKLæƒé‡: {train_config['ntkl_weight']}")
    print("="*80 + "\n")



    # è®­ç»ƒé…ç½® - æ ¹æ®Backboneé€‰æ‹©é…ç½®
    # ä¿®æ­£ï¼šä¿æŒæ–œæ ï¼Œä¸ JSON æ–‡ä»¶é”®æ ¼å¼ä¸€è‡´
    config_key = f"{args.dataset}_{args.teacher_backbone}"

    if config_key not in train_config_data:
        config_key_no_slash = f"{args.dataset}_{args.teacher_backbone.replace('/', '')}"
        if config_key_no_slash in train_config_data:
            config_key = config_key_no_slash
        else:
            config_key = f"{args.dataset}_ViT-L/14"
            print(f"âš ï¸  æœªæ‰¾åˆ° {args.dataset}_{args.teacher_backbone} çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½® {config_key}")
    else:
        print(f"âœ… ä½¿ç”¨é…ç½®: {config_key}")

    train_config = train_config_data[config_key]
    
    if not args.output_dir:
        teacher_name = args.teacher_backbone.replace('/', '-')
        student_name = args.student_backbone.replace('/', '-')
        backbone_suffix = f"_T{teacher_name}_S{student_name}"
        
        # ç®€åŒ–ç­–ç•¥åç¼€
        if args.training_strategy:
            # ç»Ÿè®¡å„æ¨¡å¼çš„æ•°é‡
            from collections import Counter
            mode_counts = Counter(args.training_strategy)
            strategy_suffix = f"_st_{''.join([f'{k}{v}' for k, v in sorted(mode_counts.items())])}"
        else:
            strategy_suffix = ""
        
        # ç®€åŒ–æ•™å¸ˆåºåˆ—åç¼€
        if args.choose_teacher_model:
            teacher_nums = args.choose_teacher_model.replace(',', '')
            # åªä¿ç•™å‰3ä¸ªå’Œå3ä¸ªæ•™å¸ˆç¼–å·
            if len(teacher_nums) > 6:
                teacher_suffix = f"_tea_{teacher_nums[:3]}..{teacher_nums[-3:]}"
            else:
                teacher_suffix = f"_tea_{teacher_nums}"
        else:
            teacher_suffix = ""
        
        # ç®€åŒ–logitsåç¼€
        logits_suffix = f"_lg_{''.join(map(str, choose_logits))}"
        
        # ç®€åŒ–è‡ªé€‚åº”å‚æ•°åç¼€
        if args.use_adaptive_distillation:
            adaptive_suffix = "_adp"
            temp_suffix = f"_T{int(temp_range[0])}-{int(temp_range[1])}"
            alpha_suffix = f"_A{int(alpha_range[0]*10)}-{int(alpha_range[1]*10)}"
        else:
            adaptive_suffix = ""
            temp_suffix = ""
            alpha_suffix = ""
        
        # ç»„åˆæˆç®€çŸ­çš„ç›®å½•å
        args.output_dir = os.path.join(
            'output', 
            args.dataset,
            f"Dual{backbone_suffix}_e{train_config['epochs']}_lr{train_config['lr']:.6f}"
            f"{strategy_suffix}{teacher_suffix}{logits_suffix}{adaptive_suffix}{temp_suffix}{alpha_suffix}"
        )
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®éšæœºç§å­
    seed = args.seed
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    cudnn.benchmark = True
    
    # æ„å»ºæ•°æ®é›†
    batch_size = train_config["model_patch_size"]
    dataset_train, len_original = build_dataset(is_train=True, args=args)
    
    print(f"\nè®­ç»ƒé›†ä¿¡æ¯:")
    print(f"  åŸå§‹é•¿åº¦: {len_original}")
    print(f"  å®é™…é•¿åº¦: {len(dataset_train)}")
    
    sampler_train = torch.utils.data.RandomSampler(dataset_train)
    data_loader_train = torch.utils.data.DataLoader(
        dataset_train, 
        sampler=sampler_train,
        batch_size=batch_size,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=False,
    )
    len_data_loader_train = len(data_loader_train)
    args.len_original = len_original
    
    # éªŒè¯é›†
    dataset_val, _ = build_dataset(is_train=False, args=args)  
    sampler_val = torch.utils.data.SequentialSampler(dataset_val)
    data_loader_val = torch.utils.data.DataLoader(
        dataset_val, 
        sampler=sampler_val,
        batch_size=4*batch_size,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    # ç¡®ä¿åœ¨åˆ›å»ºæ¨¡å‹å‰è®¾ç½®æ­£ç¡®çš„è®¾å¤‡
    with torch.cuda.device(device):
        print(f"\nğŸ”§ åœ¨è®¾å¤‡ {device} ä¸Šåˆ›å»ºåŒæ¨¡å‹ç®¡ç†å™¨...")
        # åˆ›å»ºåŒæ¨¡å‹ç®¡ç†å™¨
        dual_model = DualModelManager(args)
        dual_model.to(device) 
        print(f"âœ… åŒæ¨¡å‹å·²æˆåŠŸç§»åŠ¨åˆ°è®¾å¤‡ {device}")
    
    args.nb_classes = len(dual_model.teacher.classnames)
    
    print(f"\nåŒæ¨¡å‹ä¿¡æ¯:")
    print(f"  ç±»åˆ«æ•°: {args.nb_classes}")
    teacher_model_name = dual_model.teacher.model.visual.__class__.__name__
    student_model_name = dual_model.student.model.visual.__class__.__name__
    print(f"  æ•™å¸ˆæ¨¡å‹æ¶æ„: {teacher_model_name}")
    print(f"  å­¦ç”Ÿæ¨¡å‹æ¶æ„: {student_model_name}")
    
    # æ£€æŸ¥ç°æœ‰æ•™å¸ˆæƒé‡å’Œä½¿ç”¨è®¡åˆ’
    print("\n" + "="*80)
    print("æ£€æŸ¥ç°æœ‰æ•™å¸ˆæƒé‡...")
    available_sequences = get_available_teacher_sequences(args.dataset)
    if available_sequences:
        print(f"å¯ç”¨çš„æ•™å¸ˆæƒé‡åºåˆ—: {available_sequences}")
        for seq in available_sequences:
            print(f"  âœ… {seq}T.pth")
    else:
        print("  ğŸ“ æš‚æ— å¯ç”¨çš„æ•™å¸ˆæƒé‡")
    
    # æ£€æŸ¥æ•™å¸ˆä½¿ç”¨è®¡åˆ’çš„å¯è¡Œæ€§
    if teacher_usage_plan:
        missing_teachers = []
        for epoch, seq_num in teacher_usage_plan.items():
            if seq_num not in available_sequences:
                missing_teachers.append(seq_num)
        
        if missing_teachers:
            missing_teachers = sorted(set(missing_teachers))
            print(f"âš ï¸  è­¦å‘Š: è®¡åˆ’ä½¿ç”¨ä½†ä¸å­˜åœ¨çš„æ•™å¸ˆæƒé‡: {missing_teachers}")
            print("   è¿™äº›epochå°†ä½¿ç”¨å½“å‰æ•™å¸ˆæ¨¡å‹çŠ¶æ€")
        else:
            print("âœ… æ‰€æœ‰è®¡åˆ’ä½¿ç”¨çš„æ•™å¸ˆæƒé‡éƒ½å·²å‡†å¤‡å°±ç»ª")
    
    print("="*80 + "\n")
    
    # æ„å»ºè®°å¿†åº“
    print("æ„å»ºæ•™å¸ˆæ¨¡å‹è®°å¿†åº“...")
    teacher_memory_args = copy.deepcopy(args)
    teacher_memory_args.clip_model = args.teacher_backbone  # âœ… ä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„backbone
    teacher_center, teacher_memory = build_memory(
        teacher_memory_args, 
        dual_model.teacher,
        args.dataset, 
        data_loader_train, 
        len_original, 
        dual_model.teacher.model.embed_dim
    )
    dual_model.teacher.center_init_fixed(teacher_center)

    print("æ„å»ºå­¦ç”Ÿæ¨¡å‹è®°å¿†åº“...")
    student_memory_args = copy.deepcopy(args)
    student_memory_args.clip_model = args.student_backbone  # âœ… ä½¿ç”¨å­¦ç”Ÿæ¨¡å‹çš„backbone
    student_center, student_memory = build_memory(
        student_memory_args, 
        dual_model.student, 
        args.dataset, 
        data_loader_train, 
        len_original, 
        dual_model.student.model.embed_dim
    )
    dual_model.student.center_init_fixed(student_center)

    
    prob_list = []
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    teacher_optimizer, student_optimizer = setup_optimizers(dual_model, train_config)
    
    # åˆå§‹åŒ–èƒ½åŠ›ç›‘æ§æ¢é’ˆï¼ˆå¯é€‰ï¼‰
    capability_probe = CapabilityGapProbe(
        alpha=0.1,
        cooldown_epochs=3,
        ema_decay=0.9
    )
    
    # è®­ç»ƒé…ç½®
    args.lr = train_config['lr']
    args.min_lr = args.min_lr * 2
    args.epochs = train_config['epochs']
    args.eval_freq = train_config['eval_freq']
    
    # å¦‚æœä½¿ç”¨è‡ªå®šä¹‰ç­–ç•¥ï¼Œç¡®ä¿epochsè¶³å¤Ÿ
    if args.training_strategy:
        min_epochs = len(args.training_strategy)
        if args.epochs < min_epochs:
            print(f"è­¦å‘Š: è®¾ç½®çš„epochs ({args.epochs}) å°äºç­–ç•¥é•¿åº¦ ({min_epochs})ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º {min_epochs}")
            args.epochs = min_epochs
    
    n_parameters_teacher = sum(p.numel() for p in dual_model.teacher.parameters() if p.requires_grad)
    n_parameters_student = sum(p.numel() for p in dual_model.student.parameters() if p.requires_grad)
    
    print('-----------------------------------------------------------------------')
    print(f'Teacher parameters: {n_parameters_teacher}')
    print(f'Student parameters: {n_parameters_student}')
    print('-----------------------------------------------------------------------')
    
    loss_scaler = None
    amp_autocast = suppress
    
    # å­¦ä¹ ç‡è°ƒåº¦
    num_training_steps_per_epoch = len_data_loader_train
    lr_schedule_values = utils.cosine_scheduler(
        args.lr, args.min_lr, args.epochs, num_training_steps_per_epoch,
        warmup_epochs=args.warmup_epochs, warmup_steps=args.warmup_steps,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nå¼€å§‹åŒæ¨¡å‹è‡ªé€‚åº”è’¸é¦è®­ç»ƒï¼Œæ€»å…± {args.epochs} ä¸ªepochs")
    if args.training_strategy:
        print("\nè®­ç»ƒç­–ç•¥æ¦‚è§ˆ:")
        for i, mode in enumerate(args.training_strategy):
            if i < args.epochs:
                mode_desc = {
                    'T': 'æ•™å¸ˆè®­ç»ƒ',
                    'S': 'å­¦ç”Ÿ(DPA)',
                    'K1': 'å­¦ç”Ÿ(è‡ªé€‚åº”è’¸é¦)' if args.use_adaptive_distillation else 'å­¦ç”Ÿ(æ ‡å‡†è’¸é¦)',
                    'K2': 'å­¦ç”Ÿ(è‡ªé€‚åº”è’¸é¦+DPA)' if args.use_adaptive_distillation else 'å­¦ç”Ÿ(æ ‡å‡†è’¸é¦+DPA)'
                }
                teacher_info = f" [ä½¿ç”¨æ•™å¸ˆ{teacher_usage_plan.get(i, 'å½“å‰çŠ¶æ€')}]" if i in teacher_usage_plan else ""
                print(f"  Epoch {i}: {mode_desc[mode]}{teacher_info}")
    
    start_time = time.time()
    max_teacher_accuracy = 0.0
    max_student_accuracy = 0.0
    
    evaluation_accuracies = []
    
    for epoch in range(args.start_epoch, args.epochs):
        # ç¡®å®šå½“å‰epochçš„è®­ç»ƒæ¨¡å¼
        current_mode = dual_model.should_switch_mode(epoch)
        
        print(f"\n{'='*80}")
        print(f"Epoch {epoch}: è®­ç»ƒæ¨¡å¼ {current_mode}")
        
        # å¤„ç†æ•™å¸ˆè®­ç»ƒæ¨¡å¼
        if current_mode == 'T':
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è®­ç»ƒæ•™å¸ˆæ¨¡å‹
            if epoch in teacher_training_plan:
                target_sequence = teacher_training_plan[epoch]
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥åºåˆ—å·çš„æ•™å¸ˆæƒé‡
                if check_and_load_existing_teacher_weights_by_sequence(dual_model, args.dataset, target_sequence):
                    print(f"ğŸš€ è·³è¿‡æ•™å¸ˆè®­ç»ƒï¼Œä½¿ç”¨å·²æœ‰æƒé‡ (åºåˆ—å·: {target_sequence})")
                    
                    # ç›´æ¥è¿›è¡Œè¯„ä¼°
                    test_stats = evaluate_dual(data_loader_val, dual_model, device)
                    teacher_acc = test_stats['teacher_acc']
                    student_acc = test_stats['student_acc']
                    
                    print(f"Epoch {epoch} ({current_mode}) - Teacher Acc: {teacher_acc:.1f}%, Student Acc: {student_acc:.1f}% (ä½¿ç”¨å·²æœ‰æƒé‡)")
                    
                    # è®°å½•è¯„ä¼°ç»“æœ
                    evaluation_accuracies.append({
                        'epoch': epoch,
                        'teacher_acc': teacher_acc,
                        'student_acc': student_acc,
                        'training_mode': current_mode,
                        'teacher_sequence': target_sequence
                    })
                    
                    # è®°å½•åˆ°æ—¥å¿—
                    log_data = {
                        'epoch': epoch,
                        'teacher_acc': teacher_acc,
                        'student_acc': student_acc,
                        'training_mode': current_mode,
                        'train_loss': 0.0,
                        'skipped_training': True,
                        'teacher_sequence': target_sequence
                    }
                    
                    with open(log_path, mode="a", encoding="utf-8") as f:
                        f.write(json.dumps(log_data) + "\n")
                    
                    # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
                    if max_teacher_accuracy < teacher_acc:
                        max_teacher_accuracy = teacher_acc
                        if args.output_dir:
                            utils.save_model(args=args, model=dual_model.teacher, 
                                           model_without_ddp=dual_model.teacher, 
                                           optimizer=teacher_optimizer,
                                           loss_scaler=loss_scaler, epoch="best_teacher")
                    
                    if max_student_accuracy < student_acc:
                        max_student_accuracy = student_acc
                        if args.output_dir:
                            utils.save_model(args=args, model=dual_model.student, 
                                           model_without_ddp=dual_model.student, 
                                           optimizer=student_optimizer,
                                           loss_scaler=loss_scaler, epoch="best_student")
                    
                    print('-----------------------------------------------------------------------')
                    print(f'Max Teacher accuracy: {max_teacher_accuracy:.2f}%')
                    print(f'Max Student accuracy: {max_student_accuracy:.2f}%')
                    print('-----------------------------------------------------------------------')
                    
                    continue
                else:
                    print(f"ğŸ“ å¼€å§‹è®­ç»ƒæ•™å¸ˆæ¨¡å‹ (Epoch {epoch}, å°†ä¿å­˜ä¸º {target_sequence}T.pth)")
            else:
                print(f"â­ï¸  å½“å‰epochæ— éœ€è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œè·³è¿‡ Epoch {epoch}")
                
                # è¿›è¡Œè¯„ä¼°
                test_stats = evaluate_dual(data_loader_val, dual_model, device)
                teacher_acc = test_stats['teacher_acc']
                student_acc = test_stats['student_acc']
                
                print(f"Epoch {epoch} ({current_mode}) - Teacher Acc: {teacher_acc:.1f}%, Student Acc: {student_acc:.1f}% (è·³è¿‡è®­ç»ƒ)")
                
                log_data = {
                    'epoch': epoch,
                    'teacher_acc': teacher_acc,
                    'student_acc': student_acc,
                    'training_mode': current_mode,
                    'train_loss': 0.0,
                    'skipped_epoch': True
                }
                
                with open(log_path, mode="a", encoding="utf-8") as f:
                    f.write(json.dumps(log_data) + "\n")
                
                continue
        
        # å¤„ç†å­¦ç”Ÿè®­ç»ƒæ¨¡å¼
        elif current_mode in ['K1', 'K2']:
            teacher_loaded = False
            used_teacher_sequence = None
            
            # å¦‚æœæœ‰æ•™å¸ˆä½¿ç”¨è®¡åˆ’ï¼ŒæŒ‰è®¡åˆ’åŠ è½½
            if epoch in teacher_usage_plan:
                target_sequence = teacher_usage_plan[epoch]
                success = load_teacher_weights_by_sequence(dual_model.teacher, args.dataset, target_sequence)
                if success:
                    print(f"ğŸ”„ æŒ‰è®¡åˆ’ä½¿ç”¨æ•™å¸ˆæƒé‡ {target_sequence}T.pth è¿›è¡Œè’¸é¦")
                    teacher_loaded = True
                    used_teacher_sequence = target_sequence
                else:
                    print(f"âš ï¸  æ— æ³•åŠ è½½è®¡åˆ’çš„æ•™å¸ˆæƒé‡ {target_sequence}T.pthï¼Œä½¿ç”¨å½“å‰æ•™å¸ˆæ¨¡å‹")
            else:
                print(f"â„¹ï¸  æœªæŒ‡å®šæ•™å¸ˆæ¨¡å‹ï¼Œä½¿ç”¨å½“å‰æ•™å¸ˆæ¨¡å‹çŠ¶æ€è¿›è¡Œè’¸é¦")
        
        print(f"{'='*80}\n")
        
        # æ‰§è¡Œè®­ç»ƒ
        train_stats, teacher_memory, student_memory, prob_list = train_one_epoch_dual(
            args, dual_model,
            data_loader_train, teacher_optimizer, student_optimizer, 
            amp_autocast, device, epoch,
            loss_scaler=loss_scaler,
            lr_schedule_values=lr_schedule_values,
            train_config=train_config,
            start_steps=epoch * num_training_steps_per_epoch,
            teacher_memory=teacher_memory,
            student_memory=student_memory,
            prob_list=prob_list,
            capability_probe=capability_probe
        )
        
        # å¦‚æœæ˜¯æ•™å¸ˆè®­ç»ƒä¸”åœ¨è®­ç»ƒè®¡åˆ’ä¸­ï¼Œä¿å­˜æƒé‡
        saved_sequence = None
        if current_mode == 'T' and epoch in teacher_training_plan:
            target_sequence = teacher_training_plan[epoch]
            save_dir = Path(f"./pth0/{args.dataset}")
            save_dir.mkdir(parents=True, exist_ok=True)
            save_path = save_dir / f"{target_sequence}T.pth"
            
            torch.save({
                'model_state_dict': dual_model.teacher.state_dict(),
                'sequence_num': target_sequence,
                'training_mode': current_mode,
                'center': dual_model.teacher.center.clone() if hasattr(dual_model.teacher, 'center') else None
            }, save_path)
            
            print(f"âœ… æ•™å¸ˆæ¨¡å‹æƒé‡å·²ä¿å­˜: {save_path} (åºåˆ—å·: {target_sequence})")
            saved_sequence = target_sequence
        
        # è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹
        test_stats = evaluate_dual(data_loader_val, dual_model, device)
        
        teacher_acc = test_stats['teacher_acc']
        student_acc = test_stats['student_acc']
        
        print(f"\nEpoch {epoch} ({current_mode}) - Teacher Acc: {teacher_acc:.1f}%, Student Acc: {student_acc:.1f}%")
        
        # è®°å½•è¯„ä¼°ç»“æœ
        eval_data = {
            'epoch': epoch,
            'teacher_acc': teacher_acc,
            'student_acc': student_acc,
            'training_mode': current_mode
        }
        if saved_sequence:
            eval_data['teacher_sequence'] = saved_sequence
        if current_mode in ['K1', 'K2'] and 'used_teacher_sequence' in locals():
            eval_data['used_teacher_sequence'] = used_teacher_sequence
        
        evaluation_accuracies.append(eval_data)
        
        # åœ¨æ—¥å¿—è®°å½•ä¸­æ·»åŠ è‡ªé€‚åº”è’¸é¦ä¿¡æ¯
        log_data = {
            'epoch': epoch,
            'teacher_acc': teacher_acc,
            'student_acc': student_acc,
            'training_mode': current_mode,
            'train_loss': train_stats.get('loss', None),
        }
        
        # æ·»åŠ æ•™å¸ˆåºåˆ—ä¿¡æ¯
        if saved_sequence:
            log_data['teacher_sequence'] = saved_sequence
        if current_mode in ['K1', 'K2'] and 'used_teacher_sequence' in locals():
            log_data['used_teacher_sequence'] = used_teacher_sequence
        
        # æ·»åŠ è‡ªé€‚åº”å‚æ•°ç»Ÿè®¡åˆ°æ—¥å¿—
        for key, value in train_stats.items():
            if ('loss' in key or 'percent' in key or 'avg' in key or 
                'adaptive_' in key or 'gap_' in key) and key not in log_data:
                log_data[key] = value
        
        # ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶
        with open(log_path, mode="a", encoding="utf-8") as f:
            f.write(json.dumps(log_data) + "\n")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if max_teacher_accuracy < teacher_acc:
            max_teacher_accuracy = teacher_acc
            if args.output_dir:
                utils.save_model(args=args, model=dual_model.teacher, 
                               model_without_ddp=dual_model.teacher, 
                               optimizer=teacher_optimizer,
                               loss_scaler=loss_scaler, epoch="best_teacher")
        
        if max_student_accuracy < student_acc:
            max_student_accuracy = student_acc
            if args.output_dir:
                utils.save_model(args=args, model=dual_model.student, 
                               model_without_ddp=dual_model.student, 
                               optimizer=student_optimizer,
                               loss_scaler=loss_scaler, epoch="best_student")
        
        print('-----------------------------------------------------------------------')
        print(f'Max Teacher accuracy: {max_teacher_accuracy:.2f}%')
        print(f'Max Student accuracy: {max_student_accuracy:.2f}%')
        print('-----------------------------------------------------------------------')
        
        # ä¿å­˜è¯¦ç»†æ—¥å¿—
        log_stats = {
            **{f'train_{k}': v for k, v in train_stats.items()},
            **{f'test_{k}': v for k, v in test_stats.items()},
            'epoch': epoch,
            'n_parameters_teacher': n_parameters_teacher,
            'n_parameters_student': n_parameters_student
        }
        
        with open(os.path.join(args.output_dir, "log.txt"), mode="a", encoding="utf-8") as f:
            f.write(json.dumps(log_stats) + "\n")
    
    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print(f'\nTraining time {total_time_str}')
    
    # è¾“å‡ºæœ€ç»ˆç»“æœæ‘˜è¦
    print("\n" + "="*80)
    print("è®­ç»ƒå®Œæˆæ‘˜è¦:")
    print(f"  æœ€ä½³æ•™å¸ˆæ¨¡å‹å‡†ç¡®ç‡: {max_teacher_accuracy:.2f}%")
    print(f"  æœ€ä½³å­¦ç”Ÿæ¨¡å‹å‡†ç¡®ç‡: {max_student_accuracy:.2f}%")
    print(f"  æ€»è®­ç»ƒæ—¶é—´: {total_time_str}")
    if args.training_strategy:
        print(f"  ä½¿ç”¨çš„è®­ç»ƒç­–ç•¥: {','.join(args.training_strategy)}")
    if args.choose_teacher_model:
        print(f"  æŒ‡å®šçš„æ•™å¸ˆåºåˆ—: {args.choose_teacher_model}")
    print(f"  è’¸é¦logitsç±»å‹: {','.join(map(str, choose_logits))}")
    print(f"  ä½¿ç”¨è‡ªé€‚åº”è’¸é¦: {args.use_adaptive_distillation}")
    if args.use_adaptive_distillation:
        print(f"  æ¸©åº¦èŒƒå›´: {temp_range}")
        print(f"  AlphaèŒƒå›´: {alpha_range}")
    
    # æ˜¾ç¤ºä¿å­˜çš„æ•™å¸ˆæƒé‡ä¿¡æ¯
    pth_dir = Path(f"./pth0/{args.dataset}")
    if pth_dir.exists():
        teacher_files = list(pth_dir.glob("*T.pth"))
        if teacher_files:
            print(f"  ä¿å­˜çš„æ•™å¸ˆæƒé‡æ–‡ä»¶: {len(teacher_files)} ä¸ª")
            sorted_files = sorted(teacher_files, key=lambda x: int(x.stem.replace('T', '')))
            for file in sorted_files:
                print(f"    {file}")
    print("="*80)

if __name__ == '__main__':
    opts = get_args()
    main(opts)